# Hive 之 Java API 操作

Java 想要访问Hive，需要通过beeline的方式连接Hive，hiveserver2提供了一个新的命令行工具beeline，hiveserver2 对 之前的hive做了升级，功能更加强大，它增加了权限控制，要使用beeline需要先启动hiverserver2，再使用beeline连接


## 1、beeline 的 使用

启动hiverserver2

`$ hiveserver2`

使用beeline连接hive

`$ beeline -u jdbc:hive2://hdpcomprs:10000/db_comprs -n hadoop -p`

参数解释：
-u：连接url，可以使用IP，也可以使用主机名，端口默认为10000
-n：连接的用户名（注：不是登录hive的用户名，是hive所在服务器登录用户名）
-p：密码，可以不用输入

可以使用如下命令来修改端口

`hiveserver2 --hiveconf hive.server2.thrift.port=14000`

连接成功后，和执行hive后相同执行shell命令即可，如果想要退出连接使用 !q 或 !quit 命令。

## 2、Java API 操作 Hive

JDBC相信大家学过Java都是一个基础。Hive抽象出接口，提供JDBC连接。HIVE-JDBC其实本质上是扮演一个协议转换的角色，把jdbc的标准协议转换为访问HiveServer服务的协议。

### 2.1 Maven依赖

```java
<hadoop.version>2.6.5</hadoop.version>
<hive.version>2.1.0</hive.version>

<dependency>
        <groupId>org.apache.hive</groupId>
        <artifactId>hive-jdbc</artifactId>
        <version>${hive.version}</version>
 </dependency>
   
 <dependency> 
         <groupId>org.apache.hadoop</groupId> 
         <artifactId>hadoop-common</artifactId> 
        <version>2.4.1</version> 
 </dependency> 
    
 <dependency> 
        <groupId>jdk.tools</groupId> 
        <artifactId>jdk.tools</artifactId> 
        <version>1.6</version> 
        <scope>system</scope> 
        <systemPath>${JAVA_HOME}/lib/tools.jar</systemPath> 
    </dependency>
 
```


### 2.2 Java连接Hive代码

```java
package com.bigdata.hadoop.hive;

import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.sql.*;

/**
 * JDBC 操作 Hive（注：JDBC 访问 Hive 前需要先启动HiveServer2）
 */
public class HiveJDBC {

    private static String driverName = "org.apache.hive.jdbc.HiveDriver";
    private static String url = "jdbc:hive2://hdpcomprs:10000/db_comprs";
    private static String user = "hadoop";
    private static String password = "";

    private static Connection conn = null;
    private static Statement stmt = null;
    private static ResultSet rs = null;

    // 加载驱动、创建连接
    @Before
    public void init() throws Exception {
        Class.forName(driverName);
        Connection conn = DriverManager.getConnection(url,user,password);
        Statement stmt = conn.createStatement();
    }

    // 创建数据库
    @Test
    public void createDatabase() throws Exception {
        String sql = "create database hive_jdbc_test";
        System.out.println("Running: " + sql);
        stmt.execute(sql);
    }
 // 查询所有数据库
    @Test
    public void showDatabases() throws Exception {
        String sql = "show databases";
        System.out.println("Running: " + sql);
        rs = stmt.executeQuery(sql);
        while (rs.next()) {
            System.out.println(rs.getString(1));
        }
    }

    // 创建表
    @Test
    public void createTable() throws Exception {
        String sql = "create table emp(\n" +
                        "empno int,\n" +
                        "ename string,\n" +
                        "job string,\n" +
                        "mgr int,\n" +
                        "hiredate string,\n" +
                        "sal double,\n" +
                        "comm double,\n" +
                        "deptno int\n" +
                        ")\n" +
                     "row format delimited fields terminated by '\\t'";
        System.out.println("Running: " + sql);
        stmt.execute(sql);
    }
// 查询所有表
    @Test
    public void showTables() throws Exception {
        String sql = "show tables";
        System.out.println("Running: " + sql);
        rs = stmt.executeQuery(sql);
        while (rs.next()) {
            System.out.println(rs.getString(1));
        }
    }

    // 查看表结构
    @Test
    public void descTable() throws Exception {
        String sql = "desc emp";
        System.out.println("Running: " + sql);
        rs = stmt.executeQuery(sql);
        while (rs.next()) {
            System.out.println(rs.getString(1) + "\t" + rs.getString(2));
        }
    }

    // 加载数据
    @Test
    public void loadData() throws Exception {
        String filePath = "/home/hadoop/data/emp.txt";
        String sql = "load data local inpath '" + filePath + "' overwrite into table emp";
        System.out.println("Running: " + sql);
        stmt.execute(sql);
    }

    // 查询数据
    @Test
    public void selectData() throws Exception {
        String sql = "select * from emp";
        System.out.println("Running: " + sql);
        rs = stmt.executeQuery(sql);
        System.out.println("员工编号" + "\t" + "员工姓名" + "\t" + "工作岗位");
        while (rs.next()) {
            System.out.println(rs.getString("empno") + "\t\t" + rs.getString("ename") + "\t\t" + rs.getString("job"));
        }
    }
/ 统计查询（会运行mapreduce作业）
    @Test
    public void countData() throws Exception {
        String sql = "select count(1) from emp";
        System.out.println("Running: " + sql);
        rs = stmt.executeQuery(sql);
        while (rs.next()) {
            System.out.println(rs.getInt(1) );
        }
    }

    // 删除数据库
    @Test
    public void dropDatabase() throws Exception {
        String sql = "drop database if exists hive_jdbc_test";
        System.out.println("Running: " + sql);
        stmt.execute(sql);
    }

    // 删除数据库表
    @Test
    public void deopTable() throws Exception {
        String sql = "drop table if exists emp";
        System.out.println("Running: " + sql);
        stmt.execute(sql);
    }

    // 释放资源
    @After
    public void destory() throws Exception {
        if ( rs != null) {
            rs.close();
        }
        if (stmt != null) {
            stmt.close();
        }
        if (conn != null) {
            conn.close();
        }
    }
}

```

### 2.3 示例2

我们可以通过CLI、Client、Web UI等Hive提供的用户接口来和Hive通信，但这三种方式最常用的是CLI；Client 是Hive的客户端，用户连接至 Hive Server。在启动 Client 模式的时候，需要指出Hive Server所在节点，并且在该节点启动 Hive Server。 WUI 是通过浏览器访问 Hive。今天我们来谈谈怎么通过HiveServer来操作Hive。

Hive提供了jdbc驱动，使得我们可以用Java代码来连接Hive并进行一些类关系型数据库的sql语句查询等操作。同关系型数据库一样，我们也需要将Hive的服务打开；在Hive 0.11.0版本之前，只有HiveServer服务可用，你得在程序操作Hive之前，必须在Hive安装的服务器上打开HiveServer服务，如下：

`[wyp@localhost /home/q/hive-0.11.0]$ bin/hive --service hiveserver -p 10002`

```java
package com.wyp;
import java.sql.SQLException;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.Statement;
import java.sql.DriverManager;
 
public class HiveJdbcTest {
     
    private static String driverName = "org.apache.hadoop.hive.jdbc.HiveDriver";
   
    public static void main(String[] args) throws SQLException {
        try {
            Class.forName(driverName);
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
            System.exit(1);
        }
 
        Connection con = DriverManager.getConnection("jdbc:hive://localhost:10002/default", "wyp", "");
        Statement stmt = con.createStatement();
        String tableName = "wyphao";
        stmt.execute("drop table if exists " + tableName);
        stmt.execute("create table " + tableName +  " (key int, value string)");
        System.out.println("Create table success!");
        // show tables
        String sql = "show tables '" + tableName + "'";
        System.out.println("Running: " + sql);
        ResultSet res = stmt.executeQuery(sql);
        if (res.next()) {
            System.out.println(res.getString(1));
        }
 
        // describe table
        sql = "describe " + tableName;
        System.out.println("Running: " + sql);
        res = stmt.executeQuery(sql);
        while (res.next()) {
            System.out.println(res.getString(1) + "\t" + res.getString(2));
        }
        sql = "select * from " + tableName;
        res = stmt.executeQuery(sql);
        while (res.next()) {
            System.out.println(String.valueOf(res.getInt(1)) + "\t" + res.getString(2));
        }
 
        sql = "select count(1) from " + tableName;
        System.out.println("Running: " + sql);
        res = stmt.executeQuery(sql);
        while (res.next()) {
            System.out.println(res.getString(1));
        }
    }
}
```

编译上面的代码，之后就可以运行(我是在集成开发环境下面运行这个程序的)，结果如下：

```
Create table success!
Running: show tables 'wyphao'
wyphao
Running: describe wyphao
key                     int                 
value                   string              
Running: select count(1) from wyphao
0
 
Process finished with exit code 0
```
如果你想在脚本里面运行，请将上面的程序打包成jar文件，并将上面的依赖库放在/home/wyp/lib/（这个根据你自己的情况弄）中，同时加入到运行的环境变量，脚本如下：
```
#!/bin/bash
HADOOP_HOME=/home/q/hadoop-2.2.0
HIVE_HOME=/home/q/hive-0.11.0-bin
 
CLASSPATH=$CLASSPATH:
 
for i in /home/wyp/lib/*.jar ; do
    CLASSPATH=$CLASSPATH:$i
done
 
echo $CLASSPATH
/home/q/java/jdk1.6.0_20/bin/java -cp  $CLASSPATH:/export1/tmp/iteblog/OutputText.jar  com.wyp.HiveJdbcTest
```
上面是用Java连接HiveServer，而HiveServer本身存在很多问题（比如：安全性、并发性等）；针对这些问题，Hive0.11.0版本提供了一个全新的服务：HiveServer2，这个很好的解决HiveServer存在的安全性、并发性等问题。这个服务启动程序在 ${HIVE_HOME}/bin/hiveserver2 里面，你可以通过下面的方式来启动HiveServer2服务：

`$HIVE_HOME/bin/hiveserver2`
也可以通过下面的方式启动HiveServer2

`$HIVE_HOME/bin/hive --service hiveserver2`
两种方式效果都一样的。但是以前的程序需要修改两个地方，如下所示：
```
private static String driverName = "org.apache.hadoop.hive.jdbc.HiveDriver";
改为
private static String driverName = "org.apache.hive.jdbc.HiveDriver";

Connection con = DriverManager.getConnection("jdbc:hive://localhost:10002/default", "wyp", "");
改为
Connection con = DriverManager.getConnection("jdbc:hive2://localhost:10002/default", "wyp", "");
```
其他的不变就可以了。

这里顺便说说本程序所依赖的jar包，一共有以下几个：
```
hadoop-2.2.0/share/hadoop/common/hadoop-common-2.2.0.jar
$HIVE_HOME/lib/hive-exec-0.11.0.jar 
$HIVE_HOME/lib/hive-jdbc-0.11.0.jar 
$HIVE_HOME/lib/hive-metastore-0.11.0.jar  
$HIVE_HOME/lib/hive-service-0.11.0.jar   
$HIVE_HOME/lib/libfb303-0.9.0.jar   
$HIVE_HOME/lib/commons-logging-1.0.4.jar  
$HIVE_HOME/lib/slf4j-api-1.6.1.jar
```
如果你是用Maven，加入以下依赖
```
<dependency>
        <groupId>org.apache.hive</groupId>
        <artifactId>hive-jdbc</artifactId>
        <version>0.11.0</version>
</dependency>
 
<dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>2.2.0</version>
</dependency>
```
## 3、Hive UDF简单介绍

在Hive中，用户可以自定义一些函数，用于扩展HiveQL的功能，而这类函数叫做UDF（用户自定义函数）。UDF分为两大类：UDAF（用户自定义聚合函数）和UDTF（用户自定义表生成函数）。

**Hive有两个不同的接口编写UDF程序。一个是基础的UDF接口，一个是复杂的GenericUDF接口。**

1. org.apache.hadoop.hive.ql. exec.UDF 基础UDF的函数读取和返回基本类型，即Hadoop和Hive的基本类型。如，Text、IntWritable、LongWritable、DoubleWritable等。
2. org.apache.hadoop.hive.ql.udf.generic.GenericUDF 复杂的GenericUDF可以处理Map、List、Set类型。



[参考文章](<http://www.voidcn.com/article/p-suceexsl-vb.html>)